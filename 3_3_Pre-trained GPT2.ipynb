{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3502a4d0-456e-41bd-a4f9-ec853d7c7e61",
   "metadata": {},
   "source": [
    "## Core Text Generation Parameters\n",
    "Let’s pick the **GPT-2 model** as an example. It is a small transformer model that does not require a lot of computational resources but is still capable of generating high-quality text. A simple example to generate text using the GPT-2 model is as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "24397efc-4d28-4d64-a90c-82f15add7a44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0d8f9a9d6a54387a5bbd00a34f60490",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d56af4dc16164b5ba49b1bf8b677acc6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: Artificial intelligence is\n",
      "Generated Text:\n",
      "Artificial intelligence is a big deal to us. And I think to some degree it's part of the answer. Let's face it: If we can make machines do what we ask them to do, what we demand will be a different proposition than\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "# create model and tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "\n",
    "# tokenize input prompt to sequence of ids\n",
    "prompt = \"Artificial intelligence is\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "# generate output as a sequence of token ids\n",
    "output = model.generate(\n",
    "    **inputs,\n",
    "    max_length=50,\n",
    "    num_return_sequences=1,\n",
    "    temperature=1.0,\n",
    "    top_k=50,\n",
    "    top_p=1.0,\n",
    "    repetition_penalty=1.0,\n",
    "    do_sample=True,\n",
    "    pad_token_id=tokenizer.eos_token_id,\n",
    ")\n",
    "# convert token ids into text strings\n",
    "generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "print(f\"Prompt: {prompt}\")\n",
    "print(\"Generated Text:\")\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "942e0f4a-eda5-4449-accb-2a153f14c3da",
   "metadata": {},
   "source": [
    "### Experimenting with Temperature\n",
    "Given you know what the various parameters do, let’s see how the output changes when you adjust some of them.\n",
    "\n",
    "The temperature parameter has a significant impact on the creativity and randomness of the generated text. You can see its effect with the following example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f7fa6fde-b861-48b7-8f88-3460b1db87c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: The future of artificial intelligence is\n",
      "\n",
      "Temperature: 0.2\n",
      "Generated Text:\n",
      "The future of artificial intelligence is uncertain, but it is already in the works.\n",
      "\n",
      "\"The future of artificial intelligence is uncertain, but it is already in the works,\" said Richard Stallman, chief executive of the open-source software company, which is working on a new version of the AI called DeepMind. \"It's not going to be a big leap forward. It's going to be a big leap forward.\"\n",
      "\n",
      "The AI is already being tested in the lab of a company\n",
      "\n",
      "Temperature: 0.5\n",
      "Generated Text:\n",
      "The future of artificial intelligence is going to be a very interesting time for AI.\n",
      "\n",
      "AI is going to be a very interesting time for AI. The future of AI is going to be a very exciting time for AI.\n",
      "\n",
      "That's the thing about AI. It's a very interesting time. It's a very interesting time.\n",
      "\n",
      "I think the future of AI is going to be a very exciting time for AI.\n",
      "\n",
      "It's a very interesting time. It's a very\n",
      "\n",
      "Temperature: 1.0\n",
      "Generated Text:\n",
      "The future of artificial intelligence is being determined,\" wrote a blog post by James Stavridis, an MIT economics, law and policy fellow who has worked with researchers at the Harvard Business Review, the journal of the same name.\n",
      "\n",
      "Cognitive psychologists have long noted a growing gap in the relationship between intelligence and culture. But the study that brought the new conclusions into the spotlight has raised concerns in some quarters that artificial intelligence could lead to technological disasters.\n",
      "\n",
      "The findings of the Stanford study,\n",
      "\n",
      "Temperature: 1.5\n",
      "Generated Text:\n",
      "The future of artificial intelligence is going right back on the ground. It's all but guaranteed as the result of our evolution. In the next few years even smart machines become artificial in how they execute their actions and processes. This may all be for a good. Some may evolve slowly to be humans, but you never know when such a future exists and whether humans will truly succeed in creating them, especially in the realm of smart machines. This kind or any combination of the above is called being artificial,\"\n"
     ]
    }
   ],
   "source": [
    "prompt = \"The future of artificial intelligence is\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "# Generate text with different temperature values\n",
    "temperatures = [0.2, 0.5, 1.0, 1.5]\n",
    "print(f\"Prompt: {prompt}\")\n",
    "for temp in temperatures:\n",
    "    print()\n",
    "    print(f\"Temperature: {temp}\")\n",
    "    output = model.generate(\n",
    "        **inputs,\n",
    "        max_length=100,\n",
    "        num_return_sequences=1,\n",
    "        temperature=temp,\n",
    "        top_k=50,\n",
    "        top_p=1.0,\n",
    "        repetition_penalty=1.0,\n",
    "        do_sample=True,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "    )\n",
    "    generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    print(\"Generated Text:\")\n",
    "    print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dabb2bb-af0c-4d00-8c4a-e1c6e85316a2",
   "metadata": {},
   "source": [
    "### Top-K and Top-P Sampling\n",
    "The nucleus sampling parameters control how flexible you allow the model to pick the next token. Should you adjust the top_k parameter or the top_p parameter? Let’s see their effect in an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8b61fe06-03ff-4ad8-afa8-b79f86ff40ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: The best way to learn programming is\n",
      "\n",
      "Top-K = 5\n",
      "Generated Text:\n",
      "The best way to learn programming is to get a good understanding of programming. If you are new to programming then there will not be much information on programming at all. You will need a good understanding of programming, but you won't have to go far. The most important thing is to learn what you can learn.\n",
      "\n",
      "What are the basic rules for learning programming?\n",
      "\n",
      "There are two major rules you must follow when learning programming. First, you need to learn the basics of programming, and\n",
      "\n",
      "Top-K = 20\n",
      "Generated Text:\n",
      "The best way to learn programming is to try, and learn how to be productive of what you are doing. You need to find a good way. I'll give you three examples:\n",
      "\n",
      "\n",
      "The \"programming\" method I'm talking about is how you can write the software as part of a group of people doing something that they love. This is what you do when you are working on a project that is interesting and exciting in a very, very particular way. Your group can use your ideas\n",
      "\n",
      "Top-K = 50\n",
      "Generated Text:\n",
      "The best way to learn programming is to be able to write about everything from your favorite music apps to learning for your first few games.\n",
      "\n",
      "This may sound like a lot and I'm sure many would say you already know how to program. But there's a lot of stuff you can do at your own pace. And I can't take credit if I say \"you can learn programming too!!\" but sometimes it's not so easy. So this article will introduce you to a few of the great\n",
      "\n",
      "Top-P = 0.5\n",
      "Generated Text:\n",
      "The best way to learn programming is to learn programming by being able to understand what it is like to be a programmer. I've always had this feeling that I could learn something about programming by reading and doing it myself.\n",
      "\n",
      "Now, I have no idea what it is like to be a programmer, but I do know that I have a pretty good grasp of programming. I have the tools and knowledge to make my life a little easier. I can go from being a math major to a math\n",
      "\n",
      "Top-P = 0.7\n",
      "Generated Text:\n",
      "The best way to learn programming is to watch it, or read it. Learn programming by practicing it and getting started.\n",
      "\n",
      "The most important part of programming is not just programming and its principles. It is also about understanding the basic concepts of programming.\n",
      "\n",
      "Learn programming by learning. You are welcome to learn, but you will be doing so in an unstructured way, with your own hands.\n",
      "\n",
      "What is the development stage of a project?\n",
      "\n",
      "Programming is the first\n",
      "\n",
      "Top-P = 0.9\n",
      "Generated Text:\n",
      "The best way to learn programming is to understand the principles in Ruby. I've seen Ruby grow considerably over the years because of the way Ruby leads with no reliance on pointers. In this article I'm going to demonstrate how to put a Ruby object into function invocation, only to declare it in Ruby and save it to memory in the class.\n",
      "\n",
      "The first test will be to pass to class about to be compiled. The second should be the function run for the platform as a whole. We must\n"
     ]
    }
   ],
   "source": [
    "prompt = \"The best way to learn programming is\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "# Generate text with different top_k values\n",
    "top_k_values = [5, 20, 50]\n",
    "print(f\"Prompt: {prompt}\")\n",
    "\n",
    "for top_k in top_k_values:\n",
    "    print()\n",
    "    print(f\"Top-K = {top_k}\")\n",
    "    output = model.generate(\n",
    "        **inputs,\n",
    "        max_length=100,\n",
    "        num_return_sequences=1,\n",
    "        temperature=1.0,\n",
    "        top_k=top_k,\n",
    "        top_p=1.0,\n",
    "        repetition_penalty=1.0,\n",
    "        do_sample=True,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "    )\n",
    "    generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    print(\"Generated Text:\")\n",
    "    print(generated_text)\n",
    "\n",
    "# Generate text with different top_p values\n",
    "top_p_values = [0.5, 0.7, 0.9]\n",
    "for top_p in top_p_values:\n",
    "    print()\n",
    "    print(f\"Top-P = {top_p}\")\n",
    "    output = model.generate(\n",
    "        **inputs,\n",
    "        max_length=100,\n",
    "        num_return_sequences=1,\n",
    "        temperature=1.0,\n",
    "        top_k=0,\n",
    "        top_p=top_p,\n",
    "        repetition_penalty=1.0,\n",
    "        do_sample=True,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "    )\n",
    "    generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    print(\"Generated Text:\")\n",
    "    print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f0cec09-22bb-4b7b-b667-7c09c470abeb",
   "metadata": {},
   "source": [
    "### Controlling Repetition\n",
    "Repetition is a common issue in text generation. The repetition_penalty parameter helps address this by penalizing tokens that have already appeared in the generated text. Let’s see how it works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9b7d3ba2-1a10-4e88-ab3e-ab30f4ab0a73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: Once upon a time, there was a\n",
      "\n",
      "Repetition penalty: 1.0\n",
      "Generated Text:\n",
      "Once upon a time, there was a great deal of confusion and confusion about the nature of the \"new\" world. The first thing to understand is that the world is not the same as it was before. The world is not the same as it was before. The world is not the same as it was before. The world is not the same as it was before.\n",
      "\n",
      "The first thing to understand is that the world is not the same as it was before. The world is not the\n",
      "\n",
      "Repetition penalty: 1.2\n",
      "Generated Text:\n",
      "Once upon a time, there was a great deal of confusion about what to do with the information.\n",
      "A lot of people thought that it would be better if we just let them know when they are ready for their next round and then I can tell you how much more fun this is! It's been so long since my last tournament where everyone had played in one place but now everybody has got an opportunity to play against each other at some point during our run together as well!! This year will see\n",
      "\n",
      "Repetition penalty: 1.5\n",
      "Generated Text:\n",
      "Once upon a time, there was a man who came to the house of his father and said: \"I am going out with my brother. I have come here for you.\" And he went away without saying anything; but when we were in our houses together at night they saw him coming up from behind some bushes near where one had fallen down on us by accident or something like that—and it is very difficult not seeing what kind (as if) this happened!\n",
      "\"And so after having\n",
      "\n",
      "Repetition penalty: 2.0\n",
      "Generated Text:\n",
      "Once upon a time, there was a great deal of confusion about what to do with the money.\n",
      "The first thing that came out when I got my hands on it is this: \"What if you don't want me?\" It's not like we're going anywhere and asking for your help or anything; instead our goal here at D&D Club has always been just getting people interested in Dungeons & Dragons! We've had players come up through all sorts (and sometimes even into) other genres\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Once upon a time, there was a\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "# Generate text with different repetition penalties\n",
    "penalties = [1.0, 1.2, 1.5, 2.0]\n",
    "print(f\"Prompt: {prompt}\")\n",
    "for penalty in penalties:\n",
    "    print()\n",
    "    print(f\"Repetition penalty: {penalty}\")\n",
    "    output = model.generate(\n",
    "        **inputs,\n",
    "        max_length=100,\n",
    "        num_return_sequences=1,\n",
    "        temperature=0.3,\n",
    "        top_k=50,\n",
    "        top_p=1.0,\n",
    "        repetition_penalty=penalty,\n",
    "        do_sample=True,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "    )\n",
    "    generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    print(\"Generated Text:\")\n",
    "    print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e870572d-5aee-42f3-b337-01db6ab89931",
   "metadata": {},
   "source": [
    "### Greedy Decoding and Sampling\n",
    "The do_sample parameter controls whether the model uses sampling (probabilistic selection of tokens) or greedy decoding (always selecting the most probable token). Let’s compare these approaches:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2b597e0c-b628-4bee-8844-78c1709a133b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: The secret to happiness is\n",
      "\n",
      "Greedy Decoding (do_sample=False):\n",
      "Generated Text:\n",
      "The secret to happiness is to be happy.\n",
      "\n",
      "The secret to happiness is to be happy.\n",
      "\n",
      "The secret to happiness is to be happy.\n",
      "\n",
      "The secret to happiness is to be happy.\n",
      "\n",
      "The secret to happiness is to be happy.\n",
      "\n",
      "The secret to happiness is to be happy.\n",
      "\n",
      "The secret to happiness is to be happy.\n",
      "\n",
      "The secret to happiness is to be happy.\n",
      "\n",
      "The secret to happiness is to be happy.\n",
      "\n",
      "The\n",
      "\n",
      "Sampling (do_sample=True):\n",
      "Generated Text:\n",
      "The secret to happiness is to keep yourself grounded. \"\n",
      "\n",
      "Kirk's wife and son were both married.\n",
      "\n",
      "While his parents would keep going, Kirk would go to the local library. As a young man, he would go to live and study at the College of William and Mary. His father and grandfather would stay in the family home. Sometimes, Kirk would find Kirk's home to study.\n",
      "\n",
      "On April 23, 1820, Kirk sent his son to play on a library\n"
     ]
    }
   ],
   "source": [
    "prompt = \"The secret to happiness is\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "# Generate text with greedy decoding vs. sampling\n",
    "print(f\"Prompt: {prompt}\\n\")\n",
    "print(\"Greedy Decoding (do_sample=False):\")\n",
    "output = model.generate(\n",
    "    **inputs,\n",
    "    max_length=100,\n",
    "    num_return_sequences=1,\n",
    "    temperature=1.0,\n",
    "    top_k=50,\n",
    "    top_p=1.0,\n",
    "    repetition_penalty=1.0,\n",
    "    do_sample=False,\n",
    "    pad_token_id=tokenizer.eos_token_id,\n",
    ")\n",
    "generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "print(\"Generated Text:\")\n",
    "print(generated_text)\n",
    "print()\n",
    "print(\"Sampling (do_sample=True):\")\n",
    "output = model.generate(\n",
    "    **inputs,\n",
    "    max_length=100,\n",
    "    num_return_sequences=1,\n",
    "    temperature=1.0,\n",
    "    top_k=50,\n",
    "    top_p=1.0,\n",
    "    repetition_penalty=1.0,\n",
    "    do_sample=True,\n",
    "    pad_token_id=tokenizer.eos_token_id,\n",
    ")\n",
    "generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "print(\"Generated Text:\")\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5934a88-a8bb-4982-8022-ad9b2614402c",
   "metadata": {},
   "source": [
    "### Learning through prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a32f9ccb-6cf7-45e1-874e-4995c159384f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "import torch\n",
    "\n",
    "# Load GPT-2 small (117M)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a8f92167-9649-4b17-9edf-21f6306661be",
   "metadata": {},
   "outputs": [],
   "source": [
    "### generate text fro prompt ,helper func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9ad9a438-dff5-4c1c-9d9f-0273699cd6aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(prompt, max_length=50):\n",
    "    # Encode input\n",
    "    inputs = tokenizer.encode(prompt, return_tensors='pt')\n",
    "    \n",
    "    # Generate output\n",
    "    outputs = model.generate(inputs, max_length=max_length, \n",
    "                              do_sample=True,  # sampling to make it creative\n",
    "                              top_k=50,        # limits sampling pool\n",
    "                              top_p=0.95,      # nucleus sampling\n",
    "                              temperature=0.7, # creativity\n",
    "                              pad_token_id=tokenizer.eos_token_id)\n",
    "    \n",
    "    # Decode and print\n",
    "    text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    print(text[len(prompt):])  # Only show new generated part\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dab9cc09-fef1-4957-92b2-83fcd13e6946",
   "metadata": {},
   "source": [
    "**1 Zero shot example**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aea095bd-a6e0-4527-b8b6-bee222dc8d69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zero-shot Output:\n",
      ", I'm John.\n",
      "\n",
      "Thank you for visiting my blog. I know it's very difficult and hard to write for people who are not fluent in English. I've tried to write English for others,\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Translate English to French:\\nHello\"\n",
    "print(\"Zero-shot Output:\")\n",
    "generate_text(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fdeff66-f1ec-418f-9c8b-34622090beea",
   "metadata": {},
   "source": [
    "**2 One Shot**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "05e94b1f-2a96-40e7-a265-a28c16ddb13f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One-shot Output:\n",
      " Bonjour\n",
      "Hello -> Bonjour\n",
      "Hello -> Bonjour\n",
      "Hello -> Bonjour\n",
      "Hello -> Bonjour\n",
      "Hello -> Bonjour\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"Translate English to French:\n",
    "Good morning -> Bonjour\n",
    "Hello ->\"\"\"\n",
    "print(\"One-shot Output:\")\n",
    "generate_text(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca5bfb2f-ee71-4195-9006-01f6eafba942",
   "metadata": {},
   "source": [
    "**3 Few-shot Example**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a0381309-a3c2-4154-a656-309cd706135d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Few-shot Output:\n",
      " Béat\n",
      "Good night -> Béat nice day -> Bonne\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"Translate English to French:\n",
    "Good morning -> Bonjour\n",
    "Good night -> Bonne nuit\n",
    "How are you -> Comment ça va\n",
    "nice day ->\"\"\"\n",
    "print(\"Few-shot Output:\")\n",
    "generate_text(prompt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2178999-7a4d-432a-930c-aba03036ce36",
   "metadata": {},
   "source": [
    "GPT-2 is not perfect at zero-shot because it wasn't trained for that.\n",
    "\n",
    "It gets better with few-shot examples.\n",
    "\n",
    "You may need to adjust max_length, temperature, top_k, top_p for better outputs.\n",
    "\n",
    "Sometimes repeating generation gives better outputs due to randomness."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
